.idea/
whatap
<<<<<<< HEAD
# Created by https://www.toptal.com/developers/gitignore/api/pycharm
# Edit at https://www.toptal.com/developers/gitignore?templates=pycharm
### PyCharm ###
# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider
# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839
# User-specific stuff
.idea/**/workspace.xml
.idea/**/tasks.xml
.idea/**/usage.statistics.xml
.idea/**/dictionaries
.idea/**/shelf
# AWS User-specific
.idea/**/aws.xml
# Generated files
.idea/**/contentModel.xml
# Sensitive or high-churn files
.idea/**/dataSources/
.idea/**/dataSources.ids
.idea/**/dataSources.local.xml
.idea/**/sqlDataSources.xml
.idea/**/dynamic.xml
.idea/**/uiDesigner.xml
.idea/**/dbnavigator.xml
# Gradle
.idea/**/gradle.xml
.idea/**/libraries
# Gradle and Maven with auto-import
# When using Gradle or Maven with auto-import, you should exclude module files,
# since they will be recreated, and may cause churn.  Uncomment if using
# auto-import.
# .idea/artifacts
# .idea/compiler.xml
# .idea/jarRepositories.xml
# .idea/modules.xml
# .idea/*.iml
# .idea/modules
# *.iml
# *.ipr
# CMake
cmake-build-*/
# Mongo Explorer plugin
.idea/**/mongoSettings.xml
# File-based project format
*.iws
# IntelliJ
out/
# mpeltonen/sbt-idea plugin
.idea_modules/
# JIRA plugin
atlassian-ide-plugin.xml
# Cursive Clojure plugin
.idea/replstate.xml
# SonarLint plugin
.idea/sonarlint/
# Crashlytics plugin (for Android Studio and IntelliJ)
com_crashlytics_export_strings.xml
crashlytics.properties
crashlytics-build.properties
fabric.properties
# Editor-based Rest Client
.idea/httpRequests
# Android studio 3.1+ serialized cache file
.idea/caches/build_file_checksums.ser
### PyCharm Patch ###
# Comment Reason: https://github.com/joeblau/gitignore.io/issues/186#issuecomment-215987721
# *.iml
# modules.xml
# .idea/misc.xml
# *.ipr
# Sonarlint plugin
# https://plugins.jetbrains.com/plugin/7973-sonarlint
.idea/**/sonarlint/
# SonarQube Plugin
# https://plugins.jetbrains.com/plugin/7238-sonarqube-community-plugin
.idea/**/sonarIssues.xml
# Markdown Navigator plugin
# https://plugins.jetbrains.com/plugin/7896-markdown-navigator-enhanced
.idea/**/markdown-navigator.xml
.idea/**/markdown-navigator-enh.xml
.idea/**/markdown-navigator/
# Cache file creation bug
# See https://youtrack.jetbrains.com/issue/JBR-2257
.idea/$CACHE_FILE$
# CodeStream plugin
# https://plugins.jetbrains.com/plugin/12206-codestream
.idea/codestream.xml
# Azure Toolkit for IntelliJ plugin
# https://plugins.jetbrains.com/plugin/8053-azure-toolkit-for-intellij
.idea/**/azureSettings.xml
# End of https://www.toptal.com/developers/gitignore/api/pycharm
=======
.idea/
>>>>>>> temp-work
---
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04
ENV DEBIAN_FRONTEND=noninteractive
ENV NVIDIA_VISIBLE_DEVICES=void
# Install Python and pip
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python-is-python3 \
    && rm -rf /var/lib/apt/lists/*
<<<<<<< HEAD
=======
FROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-runtime
>>>>>>> temp-work
WORKDIR /app
# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt
# Copy application code
COPY *.py ./
COPY entrypoint.sh .
RUN chmod +x entrypoint.sh
# Expose the port
EXPOSE 8000
# Entrypoint configuration
ENTRYPOINT ["/app/entrypoint.sh"]
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
---
# main.py (v4 - CuPy Version)
from fastapi import FastAPI, BackgroundTasks
<<<<<<< HEAD
from typing import Dict
import time
import logging
import cupy as cp
=======
from typing import Dict, Optional
import torch
import time
import logging
import random
>>>>>>> temp-work
# --- 표준 로깅 설정 ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
)
logger = logging.getLogger(__name__)
# --- FastAPI 앱 초기화 ---
app = FastAPI(title="GPU Load Test API (CuPy Edition)")
# --- GPU 부하를 주는 핵심 함수 (CuPy) ---
def get_working_device_id():
    """사용 가능한 첫 번째 CUDA 장치 ID를 찾아서 반환합니다."""
    try:
        count = cp.cuda.runtime.getDeviceCount()
        logger.info(f"Checking {count} visible CUDA devices (CuPy)...")
        for i in range(count):
            try:
                with cp.cuda.Device(i):
                    # 작은 배열을 할당하여 접근 권한 확인
                    cp.array([1.0])
                logger.info(f"Device {i} is accessible.")
                return i
            except Exception as e:
                logger.warning(f"Device {i} is not accessible: {e}")
        raise RuntimeError("No accessible CUDA devices found among visible devices.")
    except Exception as e:
        # 드라이버 자체가 없는 경우 등
        logger.error(f"Failed to query CUDA devices: {e}")
        raise
<<<<<<< HEAD
def run_gpu_load(utilization_level: int, duration_sec: int):
    """지정된 시간 동안 목표 사용률에 맞는 GPU 부하를 발생시키는 함수 (CuPy 기반)"""

    # 기본 가용성 체크
    try:
        if cp.cuda.runtime.getDeviceCount() == 0:
            logger.error("No CUDA devices found.")
            return
    except Exception as e:
        logger.error(f"CUDA driver error: {e}")
        return
    try:
        device_id = get_working_device_id()
    except Exception as e:
        logger.error(f"Failed to find a working GPU: {e}")
        return
    logger.info(f"STARTING GPU load task: {utilization_level}% for {duration_sec}s using device {device_id}")
=======
# --- GPU 부하를 주는 핵심 함수 ---
def run_gpu_load(utilization_level: int, duration_sec: int):
    """지정된 시간 동안 목표 사용률에 맞는 GPU 부하를 발생시키는 함수"""
    logger.info(f"STARTING GPU load task: {utilization_level}% for {duration_sec}s.")
    if not torch.cuda.is_available():
        logger.error("CUDA is not available. Skipping GPU load.")
        return
    device = torch.device('cuda')
    size = 8192
    try:
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
    except Exception as e:
        logger.error(f"Failed to allocate tensors: {e}")
        return
    # Estimated work time for one matmul iteration.
    # Adjust this if the GPU is significantly faster/slower.
    work_time = 0.005
    if utilization_level >= 100:
        sleep_time = 0.0
    else:
        # Duty Cycle Formula derived from: Work / (Work + Sleep) = Util / 100
        # => Sleep = Work / (Util/100) - Work
        sleep_time = max(0.0, work_time / (utilization_level / 100.0) - work_time)
    start_time = time.time()
    while time.time() - start_time < duration_sec:
        # Compute
        a = torch.matmul(a, b)
        torch.cuda.synchronize()
        # Sleep to adjust load
        if sleep_time > 0:
            time.sleep(sleep_time)
>>>>>>> temp-work
    # 해당 장치 컨텍스트 진입
    with cp.cuda.Device(device_id):
        size = 8192  # 행렬 크기 (조정 가능)
        try:
            # 랜덤 행렬 생성
            a = cp.random.randn(size, size, dtype=cp.float32)
            b = cp.random.randn(size, size, dtype=cp.float32)
        except Exception as e:
            logger.error(f"Failed to initialize tensors on device {device_id}: {e}")
            return
        work_time = 0.005 # 작업 단위 시간 (초)

        # Sleep 시간 계산
        if utilization_level >= 100:
            sleep_time = 0
        else:
            # (Work / Util) - Work = Sleep
            # 예: 0.005 / 0.5 - 0.005 = 0.005 (50% Load)
            sleep_time = max(0, work_time / (utilization_level / 100.0) - work_time)
        start_time = time.time()

        # 부하 루프
        while time.time() - start_time < duration_sec:
            # 행렬 곱셈 (실제 연산)
            c = cp.matmul(a, b)

            # 동기화 (연산이 끝날 때까지 대기해야 부하가 유지됨)
            cp.cuda.Stream.null.synchronize()

            # 휴식 (사용률 조절)
            if sleep_time > 0:
                time.sleep(sleep_time)
        logger.info(f"FINISHED GPU load task: {utilization_level}% for {duration_sec}s.")

        # 메모리 정리
        del a, b, c
        cp.get_default_memory_pool().free_all_blocks()
# --- API 엔드포인트 정의 ---
@app.get("/")
def read_root() -> Dict[str, str]:
    logger.info("Root endpoint '/' was called.")
    return {"message": "GPU Load Test API (CuPy) is running. Use /load/{level} or /latency/{level}."}
@app.get("/load/{level}")
def trigger_load(level: int, background_tasks: BackgroundTasks, duration: int = 30) -> Dict[str, str]:
    """[비동기] GPU 부하 작업을 백그라운드에서 시작시키고 즉시 응답합니다."""
    if not (0 < level <= 100):
        return {"error": "Please provide a utilization level between 1 and 100."}
    # Random sleep to simulate realistic/jittery arrival or startup delay
    random_sleep = random.uniform(1.1, 10.0)
    time.sleep(random_sleep)
    logger.info(f"'/load/{level}' called. Scheduling a {duration}s task in the background.")
<<<<<<< HEAD

=======
    # FastAPI의 BackgroundTasks를 사용하여 즉시 응답하고, 작업은 백그라운드에서 실행
>>>>>>> temp-work
    background_tasks.add_task(run_gpu_load, level, duration)
    return {"message": f"GPU load test accepted. Will run in the background at ~{level}% for {duration}s."}
<<<<<<< HEAD
=======
# --- [개선점 2] Latency 시나리오를 위한 엔드포인트 ---
>>>>>>> temp-work
@app.get("/latency/{level}")
def trigger_latency_load(level: int, duration: int = 30) -> Dict[str, str]:
    """[동기] GPU 부하 작업을 '동기적으로' 실행하여 긴 Latency를 시뮬레이션합니 다."""
    if not (0 < level <= 100):
        return {"error": "Please provide a utilization level between 1 and 100."}
    logger.info(f"'/latency/{level}' called. Running a {duration}s task synchronously (blocking).")
<<<<<<< HEAD
=======
    # 여기서는 background_tasks를 사용하지 않고 직접 함수를 호출합니다.
    # 따라서 이 요청은 duration(초) 후에 응답이 완료됩니다.
>>>>>>> temp-work
    run_gpu_load(level, duration)
    logger.info(f"Synchronous task for '/latency/{level}' finished. Sending response.")
    return {"message": f"Synchronous GPU load test finished after {duration} seconds at ~{level}%."}
@app.get("/gpu-info")
def gpu_info():
    """GPU 정보를 반환합니다."""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        devices = []
        for i in range(device_count):
            devices.append(torch.cuda.get_device_name(i))
        return {
            "available": True,
            "count": device_count,
            "devices": devices,
            "current_device": torch.cuda.current_device(),
        }
    else:
        return {
            "available": False,
            "message": "No GPU detected",
        }
